{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 9-2 강: XOR 신경망 모델링\n",
    "\n",
    "## 학습 목표\n",
    "- XOR 신경망을 처리하기 위해 입력자료 행렬을 구성한다.\n",
    "- XOR 신경망을 처리하기 위해 가중치 행렬을 구성한다.\n",
    "- XOR 신경망 행렬의 형상을 일반화 한다.\n",
    "- XOR 신경망 객체를 코딩한다.\n",
    "\n",
    "## 학습 내용\n",
    "- 입력과 출력\n",
    "- 각 층의 노드 수\n",
    "- 가중치\n",
    "- 일반화\n",
    "- XOR 클래스 구현\n",
    "\n",
    "### Notation:\n",
    "- 본 강의에서 사용하는 기계학습의 표기법은 [Andrew Ng 교수의 강의](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/7dP6E/deep-l-layer-neural-network)에서 유래했으며, Andrew Ng 교수의 기계학습 표기법에 대한 한글 번역본은 [여기](http://taewan.kim/post/nn_notation/)를 참고하길 바랍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. XOR 신경망 모델링\n",
    "신경망을 코딩할 때, 우리가 자주 겪는 어려운 문제들 중에 하나는 행렬의 차원을 맞추어 주는 일입니다.  기계학습 프로그래머라면 누구나 겪는 디버깅 문제가 바로 행렬의 차원입니다.  이번 강의에서 행렬의 차원이 입력부터 출력까지 어떻게 변화되는지 다시 한번 살펴보는 것은 앞으로 기계학습에 관한 코딩을 할 때 든든한 기초가 될 것입니다. 또한 행렬의 차원 뿐만 아니라 이러한 과정에서 발생할 수 밖에 없는 다양한 행렬 표기에 대해서도 다루도록 하겠습니다. \n",
    "\n",
    "그러면, 우리가 앞 강의에서 $XOR$ 문제도 해결할 수 있는 다음과 같은 3층 신경망을 다루었는데, 이 신경망을 모델로 삼아 각 단계에서 행렬들이 어떻게 정해지고, 변화되는지 살펴보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-Weights.png?raw=true\" width=\"600\">\n",
    "<center>그림 1: XOR 신경망의 입력과 가중치</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 입력과 출력\n",
    "\n",
    "우리가 본 과목에서 다루는 기계학습의 문제들은 감독 분류에 해당합니다.  감독 분류 문제를 다룰 때, 우리에게 주어지는 것은 학습을 위한 입력 자료 $X$와 클래스 레이블 $y$입니다. 신경망 학습을 위하여 입력되는 한 자료$^{example}$는 여러 특성 즉 $m$개의 특성들로 구성되어 있습니다.  \n",
    "\n",
    "예를 들면, 붓꽃의 꽃잎의 길이와 너비, 꽃받침의 길이와 너비입니다.  붓꽃 자료의 4가지 특성을 근거로 이 자료가 어떤 품종의 붓꽃인지 알려주는 것이 클래스 레이블입니다.  한 개의 자료가 주어지면, 당연히 그 자료에 대한 클래스 레이블 $y$가 주어집니다. 그러므로 자료의 갯수가 n 이라면, 클래스 레이블의 크기도 $n$이 되어야 합니다.   학습을 위해서는 입력 자료들이 한 개가 아니라 여러 개 즉 $n$개 주어집니다. \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{X} \\in  \\mathbb{R}^{4\\times n} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "   x^{(1)}_1 & x^{(2)}_1  & x^{(3)}_1 \\cdots x^{(n)}_1\\\\\n",
    "   x^{(1)}_2 & x^{(2)}_2  & x^{(2)}_2 \\cdots x^{(n)}_2\\\\\n",
    "   x^{(1)}_3 & x^{(2)}_3  & x^{(2)}_3 \\cdots x^{(n)}_3\\\\\n",
    "   x^{(1)}_4 & x^{(2)}_4  & x^{(2)}_4 \\cdots x^{(n)}_4\\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align}\n",
    "\n",
    "그러면, 입력 X는 mxn 행렬로 구성할 수 있으며, 우리는 이런 형식을 ${X_{ij}^T}$이라 즐겨 부릅니다. (물론 $n \\times m$행렬로 구성할 수도 있고, 이러한 행렬 형식을 $X_{ij}$ 형식이라고 불렀습니다.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$XOR$ 예제의 경우, 입력 자료의 특성 $x_1, x_2$가 있으므로 $X$ 는 2개의 특성으로 구성된 입력 자료 $x = (x_1, x_2, ..., x_m)$가 m개가 있을 수 있는데, 여기서 m = 4입니다. 그러므로, 입력 $X$는 다음과 같이 $2 \\times 4$ 행렬이 됩니다. \n",
    "수학적으로는 다음과 같이 표기하며, 여기서 $\\mathbb{R}$은 실수$^{real \\ number}$를 나타내며, $\\in$은 집합에 속한다는 의미입니다. 입력은 사실상 정수이지만 연산이 실수를 사용하므로 표기 $\\mathbb{R}$를 유지하기로 합니다.\n",
    "\n",
    "\\begin{align} \\mathbf{X} \\in  \\mathbb{R}^{2 \\times 4} \\end{align}\n",
    "\n",
    "\\begin{align} \\mathbf{X} = \n",
    "\\begin{pmatrix} \n",
    "  x^{(1)}_1 & x^{(2)}_1  &  x^{(3)}_1 & x^{(4)}_1 \\\\\n",
    "  x^{(1)}_2 & x^{(2)}_2  &  x^{(3)}_2 & x^{(4)}_2 \\\\  \n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 1  \\end{pmatrix}  \\tag{1-a}\n",
    "\\end{align}\n",
    "\n",
    "특성 입력 자료 $X$의 각 벡터에 대한 클래스 레이블이 주어지므로, 클래스 레이블 $y$의 크기는 m이 되고, 여기서 $m = 4$입니다.  그러므로 클래스 레이블 $y$ 는 다음과 같이 $1 \\times 4$ 행렬이 됩니다.  이 클래스 레이블의 형상은 우리가 신경망 기계학습을 통해 예측하는 값 $\\hat{y}$와 일치해야 합니다. \n",
    "\n",
    "\\begin{align}  \\mathbf{y} \\in  \\mathbb{R}^{1\\times 4}  \\end{align}\n",
    "\n",
    "\\begin{align} \\mathbf{y} = \n",
    "  \\begin{pmatrix} y^{(1)} & y^{(2)}  &  \\cdots & y^{(m)} \\end{pmatrix} = \n",
    "  \\begin{pmatrix} 0 & 1 & 1 & 0 \\end{pmatrix} \n",
    "\\end{align}\n",
    "\n",
    "$XOR$ 학습자료입력 X와 출력 y는 다음과 같이 코딩할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "y = np.array([[0, 1, 1, 0]])\n",
    "print('X.shape={}, y.shape={}'.format(X.shape, y.shape))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론, 아래와 같은 자료 형식을 취할 수도 있습니다. 여기서는 각 학습 자료가 행 벡터$^{row \\ vector}$ 형식으로 나타납니다. \n",
    "\\begin{align}\n",
    "\\mathbf{X} = \\pmatrix { 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 }, \\qquad \\tag{1-b}\n",
    "\\mathbf{y} = \\pmatrix { 0 \\\\ 1 \\\\  1 \\\\  0} \\\\\n",
    "\\end{align}\n",
    "\n",
    "두 형식은 서로 전치행렬 관계입니다.  필요에 따라 행렬을 전치함으로 다른 형식의 행렬로 쉽게 전환할 수 있습니다.  입력층에는 XOR 논리를 정의할 수 있는 최소한 4 개의 자료가 있어야 하며, 각 자료는 $x_1$, $x_2$ 두 개의 특성이 있습니다.  \n",
    "\n",
    "XOR 문제를 해결하려면 최소한 하나의 은닉층과 노드 3개 필요합니다.  물론 출력층의 노드는 1개입니다.  XOR 문제의 입력과 출력의 형상을 나타내면 다음 그림과 같습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-DataShapes.png?raw=true\" width=\"600\">\n",
    "<center>그림 2: XOR 신경망의 입력과 클래스 레이블 행렬의 형상</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 2 은닉층과 출력층의 뉴론의 수\n",
    "\n",
    "입력층의 노드의 수는 주어진 자료에 의해 대개 결정이 됩니다. 예외적으로 입력 자료를 선처리하여 인위적인 특성을 만들어서 추가하거나 바이어스를 추가하기도 하지만, 대부분 입력층의 노드 수는 주어지는 경우가 많습니다.  \n",
    "\n",
    "출력층의 노드의 수도 역시 주어진 자료에 의해 결정할 수 있습니다.  $XOR$의 경우는 출력층의 노드 하나로 0와 1을 구별할 수 있으므로, 출력층 노드가 하나입니다.   \n",
    "\n",
    "그러면, 은닉층의 노드 수는 어떻게 할까요? 어려운 문제입니다. 우리가 할 수 있는 여러 가지 방법을 동원하여 은닉층의 노드 수를 예측하거나 시행 착오를 거쳐서라도 결정해야 합니다.  \n",
    "\n",
    "예를 들어, $XOR$가 아닌 $AND, NAND, OR$ 논리 문제는 2개의 노드로 해결이 가능하지만, $XOR$의 경우는 최소한 3개의 노드가 필요합니다. 이에 대해서는 우리가 앞에서 논리 회로를 사용하여 설명한 적이 있습니다. 복잡한 문제일수록 은닉층 수도 많아져야 하고 노드의 수도 많아져야 하는 것은 당연합니다. 그래서, 딥러닝이 필요한 것입니다. \n",
    "\n",
    "이러한 과정을 거쳐서 우리는 3층 신경망을 아래와 같이 구성하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-Nodes.png?raw=true\" width=\"600\">\n",
    "<center>그림 3: XOR 신경망의 층과 뉴론의 수</center>\n",
    "\n",
    "그림에서 볼 수 있듯이, 우리는 신경망의 각 과정에서 나타나는 모든 행렬들의 형상을 아직 다 검토하지 않았습니다.  이제 남은 행렬들의 모양이 어떻게 결정되는지 살펴보기로 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_x = X.shape[0]\n",
    "n_y = y.shape[0]\n",
    "n_h = 3\n",
    "np.random.seed(1)\n",
    "W1 = 2 * np.random.random((n_h, n_x)) - 1\n",
    "W2 = 2 * np.random.random((n_y, n_h)) - 1\n",
    "print(\"W1: {}\".format(W1))\n",
    "print(\"W2: {}\".format(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 가중치 행렬의 형상\n",
    "\n",
    "입력층과 은닉층 사이의 가중치 $W^{[1]}$의 형상은 입력층과 은닉층의 노드(뉴론)의 수에 의하여 자연스럽게 결정이 됩니다.   가중치의 형상은 가중치와 입력의 곱의 합산$^{product-sum}$ 즉 $W^{[1]} A^{[0]}$의 연산이 가능한 형상이어야 하므로, $W$의 형상은 $3 \\times 2$가 되어야 합니다. (이러한 표기 형식을 우리는 $W_{ij}^T$라고 부르기로 합니다.  만약 $2 \\times 3$ 형식이라면, 그 행렬을 전치해서 사용하면 됩니다.)  물론, 가중치가 결정이 되면 $W^{[1]} A^{[0]}$의 결과로 $Z^{[1]}$의 형상이 결정이 됩니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-WAZ1Shapes.png?raw=true\" width=\"400\">\n",
    "<center>그림 3: XOR 신경망의 1층 행렬의 형상</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1층 노드의 입력 $Z$에 활성화 함수를 적용할 때 활성화 함수는 행렬의 각 원소에 적용하므로, $A$의 형상은 $Z$의 형상과 동일합니다. \n",
    "은닉층과 출력층 사이의 가중치 $W^{[2]}$의 형상은 은닉층의 출력층의 노드(뉴론)의 수에 의하여 결정이 됩니다.  $W^{[2]}$의 형상은 $1 \\times 3$ 입니다.  \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-WAZ2Shapes.png?raw=true\" width=\"400\">\n",
    "<center>그림 4: XOR 신경망의 2층 행렬의 형상</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 출력층과 출력의 형상\n",
    "\n",
    "출력층의 입력 $Z^{[2]}$에 활성화 함수를 적용하여 신경망의 출력 $A^{[2]}$를 구합니다.  우리는 이를 $\\hat{y}$이라고 부릅니다. 이 형상은 클래스 레이블 $y$의 형상과 모두 일치하며, $1 \\times 4$입니다.   \n",
    "\n",
    "### 1.5 신경망 행렬의 형상 일반화\n",
    "\n",
    "$XOR$ 예제를 통하여 신경망의 표기와 행렬의 형상을 살펴보았습니다.  이러한 경험을 바탕으로 신경망 행렬의 형상을 일반화하여, 다음 그림의 아래쪽에 회색 부분에 보여주고 있습니다.  \n",
    "\n",
    "- $m$: 학습 자료$^{examples}$의 수\n",
    "- $n_x$: 입력의 크기 즉 특성의 크기\n",
    "- $n_y$: 출력의 크기, 학습 자료의 수\n",
    "- $n_h$: $l$ 번째 층에 숨겨진 뉴론의 수\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-Shapes.png?raw=true\" width=\"700\">\n",
    "<center>그림 5: XOR 신경망 행렬의 형상</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 5 아래에 열거된 형상은 일반화한 공식이며, 각 기호의 의미는 다음과 같습니다. \n",
    "\n",
    "- $m$: 학습 자료의 수\n",
    "- $n_x$: 입력 자료의 크기 즉 입력 특성의 수\n",
    "- $n_y$: 출력의 크기 혹은 클래스의 수 \n",
    "- $n_h^{[l]}$: $l$ 번째 은닉층의 노드의 수 \n",
    "- $X \\in \\mathbb{R}^{n_x \\times m}$: 입력 행렬\n",
    "- $x^{(i)} \\  \\in \\mathbb{R}^{n_x}$: 컬럼벡터로 표시된 i 번째 입력 자료 \n",
    "- $y \\in \\mathbb{R}^{n_y \\times m}$: 클래스 레이블 행렬\n",
    "- $y^{(i)} \\in \\mathbb{R}^{n_y \\times m}$: i 번째 입력 자료의 출력 클래스 레이블\n",
    "- $W^{[l]} \\in \\mathbb{R}^{다음 층의 노드 수 \\times 앞 층의 노드 수}$ 가중치 행렬\n",
    "- $\\hat{y} \\in \\mathbb{R}^{n_y}$: 신경망이 예측한 결과값\n",
    "- $Z^{[l]}$: $l$ 번째 층의 Sum-Product\n",
    "- $g^{[l]}$: $l$ 번째 층의 활성화 함수\n",
    "- $A^{[l]}$: $l$ 번째 층의 출력 즉 $Z^{[l]}$에 활성화 함수를 적용한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XOR 신경망 구현\n",
    "\n",
    "앞 강의들에서 우리는 4개의 뉴론으로 구성된 신경망으로 비선형 문제(예: $XOR$ 배타적 논리합)를 해결할 수 있다는 것을 알게 되었습니다. 우리는 앞에서 복잡한 수학적 접근을 해보았는데, 이제부터는 복잡한 계산은 컴퓨터에 맡길 수 있도록 코딩을 해보길 원합니다. 그래서, 우리가 수학적으로 예측한 것이 실제로 일어나는지 보기 위해 코딩을 간단히 시작해 봅시다.\n",
    "\n",
    "$XOR$와 같이 각각 2개의 입력 노드와 3개의 은닉층의 뉴론과 1개의 출력층 노드로 구성된 작은 신경망을 이용해 작업해봅시다.  (문헌에 따라서는 입력층을 하나의 층을 간주하지 않기 때문에 뉴론으로 구성된 층만을 고려하여 2개 층으로 구성된 신경망이라고 봅니다).  \n",
    "\n",
    "XOR 모델링을 따라 다음은 XOR 신경망의 입출력과 각 층들 사이의 가중치 표기를 보여줍니다. 여기도 역시 $W^T_{ij}$형식의 표기를 사용하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/XOR-NN-Weights.png?raw=true\" width=\"600\">\n",
    "<center>그림 1: XOR 신경망의 입력과 가중치</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 입력 자료 전처리\n",
    "\n",
    "$XOR$ 학습자료는 앞에서 다루었던 붓꽃자료보다 훨씬 간단합니다.  다만, XOR 학습자료의 특성 행렬$^{feature \\ matrix}$ $\\mathbf{X} = (m \\ features, n \\ samples)$로 표기하고, 각 행은 특성을 나타내며, 각 열은 한 샘플을 나타냅니다. 그러면, $XOR$ 학습자료는 $\\mathbf{X}$는 2 개의 특성과 4개의 샘플을 가진 $2 \\times 4$ 행렬입니다.  다음은 각각의 입력 자료를 컬럼 벡터로 나타낸 형식입니다. \n",
    "\n",
    "\\begin{align} \\mathbf{X} = \n",
    "\\begin{pmatrix} \n",
    "  x^{(1)}_1 & x^{(2)}_1  &  x^{(3)}_1 & x^{(4)}_1 \\\\\n",
    "  x^{(1)}_2 & x^{(2)}_2  &  x^{(3)}_2 & x^{(4)}_2 \\\\  \n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 1  \\end{pmatrix} \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 목표값 즉 클래스 레이블 $y$는 XOR 진리표를 따르면 되므로 다음과 같습니다.  $y$의 크기는 입력의 크기와 같게 설정하였습니다. 4개의 샘플에 대한 클래스 레이블을 포함하기 때문에 $y$는 $1\\times4$ 행렬이 됩니다.  \n",
    "\\begin{align}  \\mathbf{y} = \n",
    "   \\begin{pmatrix}  y^{(1)} & y^{(2)} & y^{(3)} & y^{(4)}  \\end{pmatrix}  = \n",
    "   \\begin{pmatrix} 0 & 1 & 1 & 0 \\end{pmatrix} \n",
    "\\end{align}\n",
    "\n",
    "XOR 학습자료는 넘피를 사용하여 다음과 같이 코딩할 수 있습니다. $Y$의 형상이 $(4, )$가 아니라 $(1, 4)$임을 유의하길 바랍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])    #(1, 4), but [0, 1, 1, 0].shape = (4, ) \n",
    "print('X.shape={}, Y.shape{}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])    \n",
    "print('X.shape={}, Y.shape{}'.\n",
    "      format(X.shape, Y.shape))\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 가중치 설정\n",
    "\n",
    "입력층과 은닉층 사이의 가중치는 $\\mathbf{W}^{[1]}$, 은닉층과 출력층의 가중치는 $\\mathbf{W}^{[2]}$ 행렬로 표기할 수 있습니다. 윗첨자는 층의 일련번호를 의미하며, 아랫첨자는 두 뉴론 사이를 의미합니다. 그렇다면 입력층과 은닉층 사이의 가중치 $W^{[1]}$와 은닉층과 출력층 사이의 가중치 $W^{[2]}$의 크기는 각각 어떻게 될까요? 먼저 $W^{[1]}$은 입력층이 2개이고 은닉층이 3개이기 때문에 $3 \\times 2$가 됩니다. 또한, $W^{[2]}$은 은닉층이 3개이고 출력층이 1개이기 때문에 $1 \\times 3$이 됩니다.\n",
    "\n",
    "\n",
    "여기서도 역시 $W_{ij}^T$ 형식을 사용했습니다. \n",
    "\n",
    "\\begin{equation} W^{[1]} = \n",
    "\\begin{pmatrix} \n",
    "w_{11}^{[1]} & w_{21}^{[1]} \\\\ \n",
    "w_{12}^{[1]} & w_{22}^{[1]}  \\\\ \n",
    "w_{13}^{[1]} & w_{23}^{[1]}\n",
    "\\end{pmatrix}, \\qquad  W^{[2]} = \n",
    "\\begin{pmatrix} w_{11}^{[2]} & w_{21}^{[2]} & w_{31}^{[2]} \\end{pmatrix} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면, 가중치는 어떤 값으로 시작해야 할까요? \n",
    "\n",
    "이는 아주 좋은 질문입니다. \n",
    "-1과 1사이의 작은 난수로 가중치를 초기화할 수 있습니다. 이런 때를 위하여 넘피의 난수 발생 함수를 사용하면 쉽게 가중치 행렬을 만들 수 있습니다.  다음 코드를 참조하면 됩니다. 다만, 입력층, 은닉층, 그리고 출력층의 각 노드 수를 변수에 저장하면, 이에 따라 더 손쉽게 신경망의 크기를 조정할 수 있습니다.  \n",
    "\n",
    "사용자가 은닉층 노드의 수만 정하면, 가중치의 형상은 다음과 같이 자연스럽게 결정되는 것에 유의하십시오.  \n",
    "\n",
    "함수 `random.seed()`는 난수 발생을 항상 일정하게 하여 같은 난수를 발생할 수 있도록 하기 위하여 씨드값을 설정하는 것입니다.  코드 디버깅할 때 유용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X.shape[0]           # the size of input layer,   X.shape = (2, 4)\n",
    "n_y = Y.shape[0]           # the size of output layer, Y.shape = (1, 4)\n",
    "n_h = 3                      # the size of hidden layer\n",
    "np.random.seed(1)\n",
    "W1 = 2*np.random.random((n_h, n_x)) - 1\n",
    "W2 = 2*np.random.random((n_y, n_h)) - 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_x`은 입력층 노드의 개수, `n_h`은 은닉층 노드의 개수, 그리고 `n_y`은 출력층 노드의 개수를 각각 저장합니다.\n",
    "<span style=\"color:purple\">\n",
    "위의 코드에서 np.random 모듈에 있는 random 메소드를 사용했습니다. random 메소드는 [0.0, 1.0) 의 실수 값, 즉 0에서 1사이의 값을 반환합니다. 가중치를 초기화할 때, 앞의 범위에 2를 곱하고 1을 빼주게 됨으로써, 가중치는 [-1.0, 1.0) 의 실수 값을 반환합니다. \n",
    "</span>\n",
    "\n",
    "<span style=\"color:purple\">\n",
    "우리가 생각한대로 가중치가 초기화되었는지 확인해봅시다.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W1: {}\".format(W1))\n",
    "print(\"W2: {}\".format(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 순전파 \n",
    "다음은 순전파를 위한 연산입니다. 단지 4줄의 코딩으로 완성됩니다. \n",
    "Z1은 은닉층의 입력(Sum-Product)이며, A1은 Z1를 활성화 함수에 적용한 값으로 은닉층의 출력입니다.  그리고, 은닉층의 출력값 A1은 출력층의 입력이 되어, 출력층의 입력 Z2를 구하는데 사용합니다. Z2 역시 활성화 함수를 적용하여 최종 출력값 A2를 구함으로 순전파 연산을 완성합니다. \n",
    "\n",
    "<span style=\"color:purple\">\n",
    "순전파는 학습 과정 중의 일부이기에, 앞서 초기화한 가중치와는 다르게 반복 횟수 epoch을 정해 그만큼 반복해주어야 합니다. 아래에서 역전파 및 가중치를 수정하는 과정을 보여드린 후에, 한번에 코드로 실행해보겠습니다.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Z1 = np.dot(W1, X)             # hidden layer input\n",
    "    A1 = g(Z1)                     # hidden layer output\n",
    "    Z2 = np.dot(W2, A1)            # output layer input\n",
    "    A2 = g(Z2)                     # output layer results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/multi-layerNN-4.png?raw=true\" width=\"600\">\n",
    "<center>그림 2: 다층 신경망의 역전파</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. 역전파와 가중치 조정\n",
    "\n",
    "역전파 역시 어렵지 않게 코딩을 할 수 있습니다.  \n",
    "\n",
    "- 최종 출력값 $A2$와 클래스 레이블 $Y$로 오차 $E2$를 구합니다. \n",
    "- 은닉층의 오차 $E1$도 역전파할 때 사용할 수 있도록 계산합니다.  \n",
    "- $dZ2$는 출력층의 활성화 함수를 미분하여 출력층의 입력에 대한 기울기를 구합니다. \n",
    "- $dZ1$은 은닉층의 활성화 함수를 미분하여 은닉층의 입력에 대한 기울기를 구합니다.\n",
    "- 출력층의 입력값에서의 기울기 값으로 가중치 $W2$ 조정값을 구합니다.  \n",
    "- 은닉층의 입력값에서의 기울기 값으로 가중치 $W1$ 조정값을 구합니다. \n",
    "- 각각 구한 $W1, W2$ 조정값으로 가중치를 조정하고 이를 반복 실행합니다.  \n",
    "\n",
    "```\n",
    "    E2 = Y - A2                    # error @ output\n",
    "    E1 = np.dot(W2.T, E2)          # error @ hidden\n",
    "\n",
    "    dZ2 = E2 * g_prime(Z2)         # backprop   dZ2  \n",
    "    dZ1 = E1 * g_prime(Z1)         # backprop   dZ1 \n",
    "    \n",
    "    W2 +=  np.dot(dZ2, A1.T)       # update output layer weights\n",
    "    W1 +=  np.dot(dZ1, X.T)        # update hidden layer weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. XOR 3층 신경망 코드\n",
    "\n",
    "다음은 완성된 XOR 3층 신경망 코드입니다. 주요 변수들 정리하면 코드를 이해하는데 도움이 될 것입니다. \n",
    "\n",
    "- `g` - 활성화 함수\n",
    "- `g_prime` - 활성화 함수의 미분\n",
    "- `epochs` - 반복 횟수\n",
    "- `X` - 2개의 특성을 가진 4개의 입력 자료, (2, 4) 형상\n",
    "- `Y` - 4개의 입력 자료에 대한 클래스 레이블, (1, 4) 형상\n",
    "- `n_x` - 입력 자료에 주어진 특성의 수, X.shape[0]\n",
    "- `n_h` - 사용자가 결정하는 은닉층 노드의 수\n",
    "- `n_y` - 출력 자료의 크기, 클래스 레이블의 수, Y.shape[0]\n",
    "- `W1`, `W2` - 은닉층과 출력층의 가중치\n",
    "- `cost_` - 학습 과정의 각 반복 실행에서 발생하는 오차의 기록\n",
    "- `A0` - 입력 같음. (표기의 일괄성을 위해 삽입됨)\n",
    "- `A?` - 각 층의 노드에서 연산한 결과 값, \n",
    "- `A2` - 신경망의 예측 값 (최종 출력 값)\n",
    "- `Z?` - 각 층의 노드의 입력 값, 앞 층의 출력과 가중치를 곱하여 합산한 값\n",
    "- `E?` - 각 층에서 연산한 오차\n",
    "- `dZ?` - 각 층에서 조정해야 할 가중치 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile xor.py\n",
    "# xor.py-A very simple neural network to do exclusive or.\n",
    "# use WijT and column vector style data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "g = lambda x: 1/(1 + np.exp(-x))       # activation function\n",
    "g_prime = lambda x: g(x) * (1 - g(x))   # derivative of sigmoid\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])                #=(1, 4),  but [0, 1, 1, 0].shape = (4, ) \n",
    "\n",
    "n_x = X.shape[0]\n",
    "n_y = Y.shape[0]\n",
    "n_h = 3\n",
    "\n",
    "np.random.seed(1)\n",
    "W1 = 2*np.random.random((n_h, n_x)) - 1\n",
    "W2 = 2*np.random.random((n_y, n_h)) - 1  \n",
    "print('X.shape={}, Y.shape{}'.format(X.shape, Y.shape))\n",
    "print('n_x={}, n_h={}, n_y={}'.format(n_x, n_h, n_y))\n",
    "print('W1.shape={}, W2.shape={}'.format(W1.shape, W2.shape))\n",
    "cost_ = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    A0 = X                             # unnecessary, but to illustrate only\n",
    "    Z1 = np.dot(W1, A0)           # hidden layer input\n",
    "    A1 = g(Z1)                        # hidden layer output\n",
    "    Z2 = np.dot(W2, A1)           # output layer input\n",
    "    A2 = g(Z2)                        # output layer results\n",
    "    \n",
    "    E2 = Y - A2                       # error @ output\n",
    "    E1 = np.dot(W2.T, E2)          # error @ hidden\n",
    "    if i == 0:\n",
    "        print('E1.shape={}, E2.shape={}'.format(E1.shape, E2.shape))\n",
    "\n",
    "    dZ2 = E2 * g_prime(Z2)        # backprop      # dZ2 = E2 * A2 * (1 - A2)  \n",
    "    dZ1 = E1 * g_prime(Z1)        # backprop      # dZ1 = E1 * A1 * (1 - A1)  \n",
    "    \n",
    "    W2 +=  np.dot(dZ2, A1.T)     # update output layer weights\n",
    "    W1 +=  np.dot(dZ1, A0.T)       # update hidden layer weights\n",
    "    cost_.append(np.sum(E2 * E2))\n",
    "\n",
    "print('fit returns A2:', A2)\n",
    "\n",
    "print(\"Final prediction of all\")\n",
    "for x, yhat in zip(X.T, A2.T):\n",
    "    print(x, np.round(yhat, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">\n",
    "앞에서 설명한 내용을 코딩으로 옮겼습니다. 총 2000번의 순전파, 역전파를 진행하며 가중치를 조정했습니다. 예측값 yhat 을 확인해보니 실망스러운 결과가 나왔습니다. [0, 0] 과 [0, 1] 의 예측값은 0.031과 0.971로 XOR에 맞게 예측을 했지만, [1, 0], [1, 1] 에 대한 예측값은 0.5 정도로 학습 효과가 없어 보입니다.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:purple\">\n",
    "학습과정중에 비용함수 값을 `cost_` 리스트에 저장하였습니다. 아래의 그래프를 보면 에러값이 작아지다가 0.5 정도 되는 선에서 그만 줄어들게 되는 것을 확인할 수 있습니다.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Error Squared Sum (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(cost_)), cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Squared Sum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">\n",
    "혹시 우리가 설계한 신경망이 XOR을 학습하기에 너무 단순화되지는 않았을까요? `n_h`을 4로 늘리고 결과를 확인해보겠습니다.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "g = lambda x: 1/(1 + np.exp(-x))       # activation function\n",
    "g_prime = lambda x: g(x) * (1 - g(x))   # derivative of sigmoid\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])                #=(1, 4),  but [0, 1, 1, 0].shape = (4, ) \n",
    "\n",
    "n_x = X.shape[0]\n",
    "n_y = Y.shape[0]\n",
    "n_h = 4\n",
    "\n",
    "np.random.seed(1)\n",
    "W1 = 2*np.random.random((n_h, n_x)) - 1\n",
    "W2 = 2*np.random.random((n_y, n_h)) - 1  \n",
    "print('X.shape={}, Y.shape{}'.format(X.shape, Y.shape))\n",
    "print('n_x={}, n_h={}, n_y={}'.format(n_x, n_h, n_y))\n",
    "print('W1.shape={}, W2.shape={}'.format(W1.shape, W2.shape))\n",
    "cost_ = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    A0 = X                             # unnecessary, but to illustrate only\n",
    "    Z1 = np.dot(W1, A0)           # hidden layer input\n",
    "    A1 = g(Z1)                        # hidden layer output\n",
    "    Z2 = np.dot(W2, A1)           # output layer input\n",
    "    A2 = g(Z2)                        # output layer results\n",
    "    \n",
    "    E2 = Y - A2                       # error @ output\n",
    "    E1 = np.dot(W2.T, E2)          # error @ hidden\n",
    "    if i == 0:\n",
    "        print('E1.shape={}, E2.shape={}'.format(E1.shape, E2.shape))\n",
    "\n",
    "    dZ2 = E2 * g_prime(Z2)        # backprop      # dZ2 = E2 * A2 * (1 - A2)  \n",
    "    dZ1 = E1 * g_prime(Z1)        # backprop      # dZ1 = E1 * A1 * (1 - A1)  \n",
    "    \n",
    "    W2 +=  np.dot(dZ2, A1.T)     # update output layer weights\n",
    "    W1 +=  np.dot(dZ1, A0.T)       # update hidden layer weights\n",
    "    cost_.append(np.sum(E2 * E2))\n",
    "\n",
    "print('fit returns A2:', A2)\n",
    "\n",
    "print(\"Final prediction of all\")\n",
    "for x, yhat in zip(X.T, A2.T):\n",
    "    print(x, np.round(yhat, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">\n",
    "결과가 놀랍도록 정확합니다. 프린트된 예측 결과를 보게되면, 우리가 정의한 신경망이 XOR 함수를 학습했다는 것을 확인할 수 있습니다.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:purple\">\n",
    "아래 비용함수 값의 변화를 보게되면 `cost_`가 거의 0에 수렴하는 것을 확인할 수 있습니다. 완벽하게 학습을 한 것 같군요!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Error Squared Sum (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(len(cost_)), cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Squared Sum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강의와 관계없는 참고자료일 뿐입니다. \n",
    "####  plot_decision_regions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import joy\n",
    "\n",
    "def predict(X): \n",
    "    print('predict: W1.shape:{}, Xshape:{} '.format(W1.shape, X.shape))\n",
    "    Z1 = np.dot(W1, X.T)        # hidden layer input\n",
    "    A1 = g(Z1)                     # hidden layer output\n",
    "    Z2 = np.dot(W2, A1)        # output layer input\n",
    "    A2 = g(Z2)                     # output layer results\n",
    "    return A2\n",
    "    \n",
    "# Show the prediction results\n",
    "print(\"Final prediction of all\")\n",
    "A2 = predict(X.T)\n",
    "for x, yhat in zip(X.T, A2.T):\n",
    "    print(x, yhat)\n",
    "    \n",
    "#joy.plot_decision_regions(np.delete(X, 0, axis=1), Y.flatten(), lg)   \n",
    "# np.squeeze(Y)\n",
    "joy.plot_decision_regions(X.T, Y, predict)\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강의와 관계없는 참고자료일 뿐입니다. \n",
    "#### XOR 3층 신경망 코드(Wij and row vector style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile xor.py\n",
    "#  xor.py-A very simple neural network to do exclusive or.\n",
    "# use Wij and row vector style data as they are\n",
    "import numpy as np\n",
    "\n",
    "g = lambda x: 1/(1 + np.exp(-x))       # activation function\n",
    "g_prime = lambda x: g(x) * g(1 - x)    # derivative of sigmoid\n",
    "\n",
    "epochs = 5000\n",
    "inputLayerSize, hiddenLayerSize, outputLayerSize = 2, 4, 1\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([ [0],   [1],   [1],   [0]])\n",
    "\n",
    "#np.random.seed(4)\n",
    "W1 = np.random.uniform(size=(inputLayerSize, hiddenLayerSize))\n",
    "W2 = np.random.uniform(size=(hiddenLayerSize,outputLayerSize))\n",
    "print('X.shape={}, Y.shape{}'.format(X.shape, Y.shape))\n",
    "print('W1.shape={}, W2.shape={}'.format(W1.shape, W2.shape))\n",
    "\n",
    "for i in range(epochs):\n",
    "    Z1 = np.dot(X, W1)                # hidden layer input\n",
    "    A1 = g(Z1)                           # hidden layer output\n",
    "    Z2 = np.dot(A1, W2)               # output layer input\n",
    "    A2 = g(Z2)                           # output layer results\n",
    "    \n",
    "    E2 = Y - A2                          # error @ output\n",
    "    E1 = np.dot(E2, W2.T)             # error @ hidden\n",
    "\n",
    "    dZ2 = E2 * g_prime(Z2)        # backprop      # dZ2 = E2 * A2 * (1 - A2)  \n",
    "    dZ1 = E1 * g_prime(Z1)        # backprop      # dZ1 = E1 * A1 * (1 - A1)  \n",
    "    \n",
    "    W2 +=  np.dot(A1.T, dZ2)            # update output layer weights\n",
    "    W1 +=  np.dot(X.T, dZ1)             # update hidden layer weights\n",
    "    \n",
    "print(np.round(A2, 3))                # what have we learnt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import joy\n",
    "\n",
    "class xor_net:\n",
    "    def predict(self, X): \n",
    "        print('Xshape:{} W1.shape:{}'.format(X.shape, W1.shape))\n",
    "        Z1 = np.dot(X, W1)             # hidden layer input\n",
    "        A1 = g(Z1)                     # hidden layer output\n",
    "        Z2 = np.dot(A1, W2)            # output layer input\n",
    "        A2 = g(Z2)                     # output layer results\n",
    "        return A2\n",
    "    \n",
    "nn = xor_net()\n",
    "# Show the prediction results\n",
    "print(\"Final prediction of all\")\n",
    "Yhat = nn.predict(X)\n",
    "for x, yhat in zip(X, Yhat):\n",
    "    print(x, yhat)\n",
    "    \n",
    "#joy.plot_decision_regions(np.delete(X, 0, axis=1), Y.flatten(), lg)   # np.squeeze(Y)\n",
    "print('calling plot with X=', X)\n",
    "joy.plot_decision_regions(X, Y.flatten(), nn)\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고 자료 \n",
    "\n",
    "[1] Bengio, Yoshua. \"Practical recommendations for gradient-based training of deep architectures.\" Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 437-478.\n",
    "\n",
    "[2] LeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade.\n",
    "\n",
    "[3] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" International conference on artificial intelligence and statistics. 2010.\n",
    "\n",
    "[4] Hsu, Chih-ling., [\"Code Example of a Neural Network for The Function XOR\"](https://chih-ling-hsu.github.io/2017/08/30/NN-XOR)\n",
    "\n",
    "[5] [A Neural Network in Python, Part 1: sigmoid function, gradient descent & backpropagation](http://python3.codes/neural-network-python-part-1-sigmoid-function-gradient-descent-backpropagation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "_Rejoice always, pray continually, give thanks in all circumstances; for this is God’s will for you in Christ Jesus. (1 Thes 5:16-18)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
