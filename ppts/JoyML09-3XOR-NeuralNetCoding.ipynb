{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python\n",
    "**************\n",
    "\n",
    "# 제 09-3강: XOR NeuralNetwork Coding\n",
    "\n",
    "\n",
    "\n",
    "## 학습 목표\n",
    "- 다층 신경망을 경사하강법과 역전파 알고리즘으로 구현한다. \n",
    "- XOR로 신경망을 학습하고 테스트한다.\n",
    "\n",
    "## 학습 내용\n",
    "- 객체지향 다층 신경망 구현하기\n",
    "- fit() 메소드 \n",
    "- net_input() 메소드\n",
    "- predict() 메소드\n",
    "- XOR 신경망 학습\n",
    "\n",
    "#### Note:\n",
    "- 본 강의에서 사용하는 기계학습의 표기법은 [Andrew Ng 교수의 강의](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/7dP6E/deep-l-layer-neural-network)에서 유래했으며, Andrew Ng 교수의 기계학습 표기법에 대한 한글 번역본은 [여기](http://taewan.kim/post/nn_notation/)를 참고하길 바랍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">\n",
    "이번 강의에서는 앞에서 구현했던 신경망을 객체 지향 프로그래밍을 사용해서 다시 구현해보겠습니다. 클래스의 이름은 `NeuralNetwork` 로 합니다. 인스턴스를 초기화할 때, 신경망의 구조 `net_arch` 및 학습률 `eta`, 몇번 반복해서 학습할 것인지를 지정해주는 `epochs`, 가중치를 초기화할 때 사용할 시드 값인 `random_seed`를 지정해줍니다.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">\n",
    "`NeuralNetwork` 클래스에서는 총 5개의 메소드를 정의했습니다. 각각을 살펴보도록 하죠.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `fit` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def fit(self, X, Y):\n",
    "    \"\"\" \n",
    "    X: input dataset in row vector style, \n",
    "    Y: class labels\n",
    "    w: optional weights, its shape is (3, 1)\n",
    "    \"\"\"\n",
    "    # seed random numbers to make calculation deterministic \n",
    "    # initialize weights randomly with mean 0\n",
    "    np.random.seed(self.random_seed)\n",
    "    self.W1 = 2*np.random.random((self.net_arch[1], self.net_arch[0])) - 1\n",
    "    self.W2 = 2*np.random.random((self.net_arch[2], self.net_arch[1])) - 1      \n",
    "    print('X.shape={}, Y.shape{}'.format(X.shape, Y.shape))\n",
    "    print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "\n",
    "    self.cost_ = []\n",
    "\n",
    "    for iter in range(self.epochs):\n",
    "        A0 = X                             # unnecessary, but to illustrate only\n",
    "        Z1 = np.dot(self.W1, A0)           # hidden layer input\n",
    "        A1 = self.g(Z1)                    # hidden layer output\n",
    "        Z2 = np.dot(self.W2, A1)           # output layer input\n",
    "        A2 = self.g(Z2)                    # output layer results\n",
    "\n",
    "        E2 = Y - A2                        # error @ output\n",
    "        E1 = np.dot(self.W2.T, E2)         # error @ hidden\n",
    "\n",
    "        # multiply the error by the sigmoid slope at the values in Z? or A?\n",
    "        dZ2 = E2 * self.g_prime(Z2)        # backprop      # dZ2 = E2 * A2 * (1 - A2)  \n",
    "        dZ1 = E1 * self.g_prime(Z1)        # backprop      # dZ1 = E1 * A1 * (1 - A1)  \n",
    "\n",
    "        self.W2 +=  np.dot(dZ2, A1.T)      # update output layer weights\n",
    "        self.W1 +=  np.dot(dZ1, A0.T)      # update hidden layer weights\n",
    "        self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "    return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit` 메소드에서는 `net_arch` 에 맞게 가중치를 [-1, 1) 의 임의의 값으로 초기화합니다. 그리고 `epochs` 만큼 반복해서 학습을 하며, 가중치를 업데이트 해줍니다. 학습을 하며, 각 `epoch` 마다 비용 함수 값을 `cost_` 리스트에 저장합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `net_input`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def net_input(self, X):                     ## sum-product  z\n",
    "    if X.shape[0] == self.w.shape[0]:   # used with X0 = True data \n",
    "        return np.dot(X, self.w)\n",
    "    else:\n",
    "        return np.dot(X, self.w[1:]) + self.w[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`net_input` 은 입력값과 가중치를 내적해줍니다. 입력값에 상수 1이 포함된 열이 있는 경우와 그렇지 않은 경우를 구분해서 내적을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `g`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g` 는 활성화 함수를 의미합니다. 여기에서는 시그모이드 함수를 사용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. g_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g_prime` 은 활성화 함수를 미분한 함수입니다. 여기서는 시그모이드 함수를 미분한 함수를 넣었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict` 는 학습한 가중치를 사용해서, 기존의 입력값을 신경망에 넣고 예측하는 메소드입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드는 앞서 설명한 내용을 모아놓은 것입니다. 신경망이 성공적으로 XOR 기능을 하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# import k-mooc in-house library\n",
    "import joy\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\" This class implements a multi-perceptron with backpropagation. \n",
    "         This handles a simple logics such as OR, AND, NAND, and \n",
    "         NOR gates, including XOR.\n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch, eta=0.1, epochs=100, random_seed=1):\n",
    "        self.layers = len(net_arch)\n",
    "        self.net_arch = net_arch\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\" \n",
    "        X: input dataset in row vector style, \n",
    "        Y: class labels\n",
    "        w: optional weights, its shape is (3, 1)\n",
    "        \"\"\"\n",
    "        # seed random numbers to make calculation deterministic \n",
    "        # initialize weights randomly with mean 0\n",
    "        np.random.seed(self.random_seed)\n",
    "        self.W1 = 2*np.random.random((self.net_arch[1], self.net_arch[0])) - 1\n",
    "        self.W2 = 2*np.random.random((self.net_arch[2], self.net_arch[1])) - 1      \n",
    "        #print('X.shape={}, Y.shape{}'.format(X.shape, Y.shape))\n",
    "        #print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "\n",
    "        self.cost_ = []\n",
    "        \n",
    "        for iter in range(self.epochs):\n",
    "            A0 = X                             # to build a deep net later\n",
    "            Z1 = np.dot(self.W1, A0)           # hidden layer input\n",
    "            A1 = self.g(Z1)                    # hidden layer output\n",
    "            Z2 = np.dot(self.W2, A1)           # output layer input\n",
    "            A2 = self.g(Z2)                    # output layer results\n",
    "\n",
    "            E2 = Y - A2                        # error @ output\n",
    "            E1 = np.dot(self.W2.T, E2)         # error @ hidden\n",
    "\n",
    "            # multiply the error by the slope at the values in Z?\n",
    "            dZ2 = E2 * self.g_prime(Z2)        # backprop     \n",
    "            dZ1 = E1 * self.g_prime(Z1)        # backprop   \n",
    "           \n",
    "            self.W2 += self.eta * np.dot(dZ2, A1.T) # update output layer W\n",
    "            self.W1 += self.eta * np.dot(dZ1, A0.T) # update hidden layer W\n",
    "            self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):                     ## sum-product  z\n",
    "        if X.shape[0] == self.w.shape[0]:   # used with X0 = True data \n",
    "            return np.dot(X, self.w)\n",
    "        else:\n",
    "            return np.dot(X, self.w[1:]) + self.w[0]\n",
    "    \n",
    "    def g(self, x):    # activation function\n",
    "        return 1/(1 + np.exp((-x)))\n",
    "    \n",
    "    def g_prime(self, x):  # gradient or sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "\n",
    "    def predict(self, X): \n",
    "        #print('predict: W1.shape:{}, Xshape:{} '.format(self.W1.shape, X.shape))\n",
    "        Z1 = np.dot(self.W1, X.T)           # hidden layer input\n",
    "        A1 = self.g(Z1)                     # hidden layer output\n",
    "        Z2 = np.dot(self.W2, A1)            # output layer input\n",
    "        A2 = self.g(Z2)                     # output layer results\n",
    "        return A2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    nn = NeuralNetwork(net_arch=[2, 4, 1], eta = 0.5, epochs=10000)\n",
    "    X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])   # input dataset - col vector     \n",
    "    Y = np.array([0, 1, 1, 0])                     # class labels  - for rcolvector  \n",
    "    \n",
    "    nn.fit(X, Y)                       # train the net\n",
    "\n",
    "    print(\"Final prediction of all\")\n",
    "    A2 = nn.predict(X.T)\n",
    "    for x, yhat in zip(X.T, A2.T):\n",
    "        print(x, np.round(yhat, 3))\n",
    "        \n",
    "    joy.plot_decision_regions(X.T, Y, nn)   \n",
    "    plt.xlabel('x-axis')\n",
    "    plt.ylabel('y-axis')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예측값에 대한 그래프를 봅시다. 아주 잘 예측한 것을 확인할 수 있죠?\n",
    "\n",
    "아래의 그래프는 각 `epoch` 마다 비용함수가 어떻게 변화하는지 보여줍니다. 비용 함수의 값이 점차적으로 줄어드는 것을 확인할 수 있죠? 우리가 예상한 대로 신경망이 학습을 하고 있다는 증거입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Squared Sum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 자료 \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Bengio, Yoshua. \"Practical recommendations for gradient-based training of deep architectures.\" Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 437-478.\n",
    "\n",
    "[2] LeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade.\n",
    "\n",
    "[3] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" International conference on artificial intelligence and statistics. 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "- XOR 신경망을 코드를 이해한다.\n",
    "- XOR 신경망의 은닉층의 갯수에 따른 결과를 확인한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
