{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 11-3 강: Gradient Descent 1 - Batch GD\n",
    "\n",
    "\n",
    "## 학습목표 \n",
    "- 배치 경사하강법(Batch GD)으로 학습의 정확도를 이해한다.\n",
    "- 확률적 경사하강법(Stochastic GD)으로 학습의 효율성을 이해한다.\n",
    "\n",
    "## 학습 내용\n",
    "- MNIST 자료셋에 대한 다양한 경사하강법 비교하기\n",
    "- 배치 경사하강법(Batch GD)의 정확도 이해하기 \n",
    "- 확률적 경사하강법(Stochastic GD)으로 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import joy\n",
    "imp.reload(joy)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MNIST 배치 경사하강법 신경망의 구현\n",
    "\n",
    "우리는 앞 강의에서 MNIST 데이터셋의 분류 문제를 다룰 수 있는 신경망에 대해 공부했습니다. 또한, 신경망이 학습하는 가장 기본적인 (배치) 경사하강법에 대해 배웠습니다. 이번 장에서는 경사 하강법에 대해 복습하고, MNIST 데이터셋을 학습하는 실습을 하도록 하겠습니다.\n",
    "\n",
    "먼저 경사 하강법이란 현재 위치에서 어떤 함수의 극소점을 찾는 방법을 말합니다. 그렇다면 반대로 극대점을 찾는 방법은 경사 상승법이라고 합니다. 우리가 신경망을 학습 시킬 때에는 신경망의 예측 값과 레이블 간의 오차율을 작게 만드는 것이 목표이기 때문에 오차율의 극소점을 발견하는 경사 하강법을 사용합니다.\n",
    "\n",
    "다음과 같이 $x^2 + y^2$의 함수 그래프가 있다고 할 때, 그래프가 가장 움푹 파인 곳이 극소점이라는 것을 한 눈에 찾을 수 있을 것입니다.\n",
    "즉, 기울기가 0인 곳을 극소점/극대점 이라고 부르며 이는 그래프의 미분 값이 0인 곳을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/GradientDescentMesh.png?raw=true\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 2차원 함수보다 차원이 높은 함수일 수록 발견하기 어려울 것 입니다. 신경망에서는 복잡한 함수의 극소점을 어떻게 찾아갈까요?\n",
    "\n",
    "아래 그림과 같이 여러분이 등산을 한다고 생각해 봅시다. 아니면, 숲이 우거져 앞이 보이지 않는 밀림에 있다고 상상해 봅시다. 여러분은 지도도 나침반도 없는 상황에서 정상에 도달하여야 합니다. 현재 있는 위치도 어디인지 모르고 숲이 우거져 한치 앞도 보이지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/GradientDescentEx.png?raw=true\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 직관적인 방법으로는 현재 본인이 위치한 곳 보다 조금 더 높아 보이는 곳으로 가는 방법일 것입니다. 그리고 새로운 위치에서 다시 한번 지금 내가 갈 수 있는 가장 높아 보이는 곳으로 한 발 나아갑니다. 이런 작업을 반복하다 보면 결국 정상에 도달해 있지 않을까요?\n",
    "\n",
    "경사 하강법은 정상에 오르는 방법과 유사하게 오차율을 최소화 하는 극소점을 찾아 내려갑니다. 아래 식을 보며 조금 더 얘기해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\Delta w &= -\\eta \\Delta{J(w)} \\\\\n",
    "               &=-\\eta \\frac{\\partial{J(w)}}{\\partial{w_j}} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 $\\eta$는 학습율을 의미하며, 위의 예제에서는 여러분들의 보폭을 말합니다. 한 발 내딛을 때 얼마나 큰 보폭으로 산을 올라갈지를 결정합니다. <br>또한, 우리가 원하는 것은 오차율의 극대점을 찾는 것의 반대이기 때문에 $\\eta\\ 앞에 -$기호를 사용하여 극소점을 거꾸로 찾아갑니다.<br>산의 정상에서 내려오는 일이라고 생각하면 됩니다. 뒤에 따라오는 미분 값은 신경망의 가중치에 따른 오차 값의 변화(기울기)가 0이 되는 점을 찾는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 기본적인 경사 하강법에 대한 복습을 마쳤으니 MNIST 데이터셋을 직접 학습시켜보도록 하겠습니다. 아래는 앞에서 배운 비용함수 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "J(w) = \\frac{1}{2} \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2 \\tag1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 우리는 6만개의 MNIST 데이터 전체를 이용하여 학습하기 때문에 식(2) 처럼 데이터의 개수를 비용함수에 나누어 줘야 합니다.\n",
    "여기서는 데이터의 개수를 $m$으로 표기합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "J(w) = \\frac{1}{2m} \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2  \\tag{2} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta w &= -\\eta \\Delta{J(w)} \\\\\n",
    "               &=-\\eta \\frac{\\partial{J(w)}}{\\partial{w_j}} \\tag{3}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 신경망의 가중치는 6만개의 모든 데이터를 보고 그 중 가장 오차율을 최소화 하는 방향으로 $\\eta$만큼 한 걸음 내딛어 업데이트 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MNIST 배치 경사하강법 신경망의 구현\n",
    "\n",
    "우리는 앞 강의에서 MNIST 데이터셋의 분류 문제를 다룰 수 있는 신경망에 대해 공부습니다.  가장 기본적인 (배치) 경사하강법에 대해 알아 보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile code/MnistBGD.py\n",
    "#%load code/MnistBGD.py\n",
    "\n",
    "class MnistBGD_LS(object):\n",
    "    \"\"\" Batch Gradient Descent with Learning Schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_h1, n_y, eta = 0.1, epochs = 100, random_seed=1):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "        \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0)        # hidden layer inputs\n",
    "        A1 = self.g(Z1)                 # hidden layer outputs/activation func\n",
    "        Z2 = np.dot(self.W2, A1)        # output layer inputs\n",
    "        A2 = self.g(Z2)                 # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.cost_ = []\n",
    "        self.m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y)     \n",
    "        # learning rate is scheduled to decrement by a step of \n",
    "        # which the inteveral from self.eta to 0.0001 eqaully \n",
    "        # divided by total number of iterations(epochs or \n",
    "        # epochs * m_samples)\n",
    "        eta_scheduled = np.linspace(self.eta, 0.0001, self.epochs)\n",
    "        \n",
    "        # for momentum\n",
    "        #self.v1 = np.zeros_like(self.W1)\n",
    "        #self.v2 = np.zeros_like(self.W2)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "\n",
    "            A0 = np.array(X, ndmin=2).T       \n",
    "            Y0 = np.array(Y, ndmin=2).T     \n",
    "\n",
    "            Z1, A1, Z2, A2 = self.forpass(A0)  \n",
    "            E2 = Y0 - A2                      \n",
    "            E1 = np.dot(self.W2.T, E2)         \n",
    "\n",
    "            dZ2 = E2 * self.g_prime(Z2)          \n",
    "            dZ1 = E1 * self.g_prime(Z1)       \n",
    "            \n",
    "            # udpate weight with momentum\n",
    "            #eta = learning_schedule[epoch]\n",
    "            #self.v2 = 0.9 * self.v2 + self.eta * np.dot(dZ2, A1.T) / m_samples\n",
    "            #self.v1 = 0.9 * self.v1 + self.eta * np.dot(dZ1, A0.T) / m_samples\n",
    "            #self.W2 += self.v2     \n",
    "            #self.W1 += self.v1 \n",
    "\n",
    "            # update weights without momentum\n",
    "            # eta = eta_scheduled[epoch]\n",
    "            self.W2 +=  self.eta * np.dot(dZ2, A1.T) / self.m_samples    \n",
    "            self.W1 +=  self.eta * np.dot(dZ1, A0.T) / self.m_samples    \n",
    "            self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                 # activation_function: sigmoid\n",
    "        x = np.clip(x, -500, 500)   # prevent from overflow, \n",
    "        return 1.0/(1.0+np.exp(-x)) # stackoverflow.com/questions/23128401/\n",
    "                                    # overflow-error-in-neural-networks-implementation\n",
    "    \n",
    "    def g_prime(self, x):                    # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):       # fully vectorized calculation\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    \n",
    "    def evaluate_onebyone(self, Xtest, ytest):\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0\n",
    "        for m in range(m_samples):\n",
    "            A2 = nn.predict(Xtest[m])\n",
    "            yhat = np.argmax(A2)\n",
    "            if yhat == ytest[m]:\n",
    "                scores += 1        \n",
    "        return scores/m_samples * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "%matplotlib inline\n",
    "\n",
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()     \n",
    "X = X[:30]\n",
    "y = y[:30]\n",
    "selected = np.random.choice(X.shape[0], 200)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]\n",
    "joy.show_mnist_grid(Xtest[:5].reshape(-1, 28, 28))\n",
    "print(ytest[:5])\n",
    "\n",
    "# set hyperparameters and instantiate the class object\n",
    "n_x, n_h, n_y = 784, 100, 10          \n",
    "nn = MnistBGD_LS(n_x, n_h, n_y, eta = 0.2, epochs = 1000)  \n",
    "\n",
    "# train the model\n",
    "nn.fit(X, y)       \n",
    "accuracy = nn.evaluate(X, y)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtest, ytest = joy.getXy_mnist_csv(\"mnist_test_10.csv\", display = False)\n",
    "selected = np.random.choice(X.shape[0], 100)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]\n",
    "joy.show_mnist_grid(X[:10].reshape(-1, 28, 28))\n",
    "print(ytest[:10])\n",
    "\n",
    "accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 70%, eta=0.01, epoch=100\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 60%, eta=0.1, epoch=100\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 80%, eta=0.1, epoch=1000, learning schedule\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 70%, eta=0.1, epoch=2000, learning schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.choice(X.shape[0], 100)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]          \n",
    "\n",
    "accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()     \n",
    "# set hyperparameters and instantiate the class object\n",
    "n_x, n_h, n_y = 784, 100, 10          \n",
    "nn = MnistBGD_LS(n_x, n_h, n_y, eta = 0.2, epochs = 200)  \n",
    "# train the model\n",
    "nn.fit(X[:5000], y[:5000])       \n",
    "accuracy = nn.evaluate(Xtest[:1000], ytest[:1000])      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "%matplotlib inline\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist() \n",
    "epoch_list = [i for i in np.arange(100, 1500, 200)]\n",
    "self_accuracy = []\n",
    "test_accuracy = []\n",
    "for i, e in  enumerate(epoch_list):\n",
    "    nn = MnistBGD_LS(784, 150, 10, eta=0.1, epochs = e)  \n",
    "    nn.fit(X[:5000], y[:5000])  \n",
    "    self_accuracy.append(nn.evaluate(X[:5000], y[:5000]))  \n",
    "    test_accuracy.append(nn.evaluate(Xtest[:1000], ytest[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(epoch_list, self_accuracy, label='self')\n",
    "plt.plot(epoch_list, test_accuracy, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Early Stopping:\"Beautiful free lunch\"')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('last test acc=', test_accuracy[-1])\n",
    "accuracy = nn.evaluate(Xtest[:200], ytest[:200])      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 신속한 테스트 (훈련 자료 자체를 테스트함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "%matplotlib inline\n",
    "\n",
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()     \n",
    "#X, y = joy.read_mnist_csv(\"mnist_train_100.csv\", display = False)\n",
    "\n",
    "X = X[:30]\n",
    "y = y[:30]\n",
    "selected = np.random.choice(X.shape[0], 200)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]\n",
    "joy.show_mnist_grid(Xtest[:5].reshape(-1, 28, 28))\n",
    "print(ytest[:5])\n",
    "\n",
    "# set hyperparameters and instantiate the class object\n",
    "n_x, n_h, n_y = 784, 100, 10          \n",
    "nn = MnistBGD_LS(n_x, n_h, n_y, eta = 0.1, epochs = 1000)  \n",
    "\n",
    "# train the model\n",
    "nn.fit(X, y)       \n",
    "accuracy = nn.evaluate(X, y)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()     \n",
    "# set hyperparameters and instantiate the class object\n",
    "n_x, n_h, n_y = 784, 100, 10          \n",
    "nn = MnistBGD_LS(n_x, n_h, n_y, eta = 0.2, epochs = 200)  \n",
    "# train the model\n",
    "nn.fit(X[:5000], y[:5000])       \n",
    "accuracy = nn.evaluate(Xtest[:1000], ytest[:1000])      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "%matplotlib inline\n",
    "\n",
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()     \n",
    "\n",
    "# set hyperparameters and instantiate the class object\n",
    "n_x, n_h, n_y = 784, 100, 10          \n",
    "nn = MnistBGD_LS(n_x, n_h, n_y, eta = 0.1, epochs = 100)  \n",
    "\n",
    "# train the model\n",
    "nn.fit(X, y)       \n",
    "accuracy = nn.evaluate(X, y)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Xtest, ytest = joy.getXy_mnist_csv(\"mnist_test_10.csv\", display = False)\n",
    "\n",
    "selected = np.random.choice(X.shape[0], 100)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]\n",
    "joy.show_mnist_grid(X[:10].reshape(-1, 28, 28))\n",
    "print(ytest[:10])\n",
    "\n",
    "accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 70%, eta=0.01, epoch=100\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 60%, eta=0.1, epoch=100\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 80%, eta=0.1, epoch=1000, learning schedule\n",
    "#mnist_trian_100, mnist_test_10.csv accuraccy 70%, eta=0.1, epoch=2000, learning schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.choice(X.shape[0], 100)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]          \n",
    "\n",
    "accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 오차(self.cost_)의 시각화 \n",
    "\n",
    "신경망을 학습시키면서 발생하는 오차(손실)를 MnistMiniBatch객체의 속성 `cost_`에 저장되어 있습니다. 이를 시각화해서 신경망이 어떻게 학습을 하였는지, 손실을 최소화하는 방향을 수렴하였는지 분석할 수 있습니다.  다음 셀의 코드를 실행해 봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Squared Sum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료/테스팅을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile code/MnistBGD.py\n",
    "#%load code/MnistBGD.py\n",
    "# 이제 기본적인 경사 하강법에 대한 복습을 마쳤으니 \n",
    "# MNIST 데이터셋을 직접 학습시켜보도록 하겠습니다. \n",
    "class MnistBGD(object):\n",
    "    \"\"\" Batch Gradient Descent  \"\"\"\n",
    "    def __init__(self, n_x, n_h, n_y, eta = 0.1, epochs = 100, random_seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0)    # hidden layer inputs\n",
    "        A1 = self.g(Z1)             # hidden layer outputs/activation func\n",
    "        Z2 = np.dot(self.W2, A1)    # output layer inputs\n",
    "        A2 = self.g(Z2)             # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y)  \n",
    "        \n",
    "        self.cost_ = []\n",
    "        for epoch in range(self.epochs):\n",
    "            A0 = np.array(X, ndmin=2).T     \n",
    "            Y0 = np.array(Y, ndmin=2).T      \n",
    "\n",
    "            Z1 = np.dot(self.W1, A0)          \n",
    "            A1 = self.g(Z1)                  \n",
    "            Z2 = np.dot(self.W2, A1)       \n",
    "            A2 = self.g(Z2)                 \n",
    "\n",
    "            E2 = Y0 - A2                   \n",
    "            E1 = np.dot(self.W2.T, E2)          \n",
    "\n",
    "            dZ2 = E2 * self.g_prime(Z2)       \n",
    "            dZ1 = E1 * self.g_prime(Z1)  \n",
    "            \n",
    "            dW2 = self.eta * np.dot(dZ2, A1.T)\n",
    "            dW1 = self.eta * np.dot(dZ1, A0.T)\n",
    "\n",
    "            self.W2 += dW2 / self.m_samples   \n",
    "            self.W1 += dW1 / self.m_samples    \n",
    "\n",
    "            self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                 # activation_function: sigmoid\n",
    "        x = np.clip(x, -500, 500)   # prevent from overflow, \n",
    "        return 1.0/(1.0+np.exp(-x)) # stackoverflow.com/questions/23128401/\n",
    "                                    # overflow-error-in-neural-networks-implementation\n",
    "    \n",
    "    def g_prime(self, x):           # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):      \n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        print(A2.shape)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    \n",
    "    def evaluate_onebyone(self, Xtest, ytest):\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0\n",
    "        for m in range(m_samples):\n",
    "            A2 = nn.predict(Xtest[m])\n",
    "            yhat = np.argmax(A2)\n",
    "            if yhat == ytest[m]:\n",
    "                scores += 1        \n",
    "        return scores/m_samples * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist()     \n",
    "X, y = X[:1000], y[:1000]\n",
    "Xtest, ytest = Xtest[:200], ytest[:200]  \n",
    "epoch_list = np.arange(2000, 5001, 250)\n",
    "for epoch in epoch_list:\n",
    "    nn = MnistBGD(784, 100, 10, eta = 0.1, epochs = epoch).fit(X, y)   \n",
    "    self_accuracy = nn.evaluate(X, y)  \n",
    "    test_accuracy = nn.evaluate(Xtest, ytest)      \n",
    "    print('epoch:{}, Accuracy self:{}, test:{}%'.\n",
    "          format(epoch, np.round(self_accuracy, 2), test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.title('MNIST BGD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "%matplotlib inline\n",
    "\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()\n",
    "print(Xtest.shape)\n",
    "nn = MnistBGD_LS(784, 100, 10, eta = 0.1, epochs = 200)  \n",
    "nn.fit(X, y)       \n",
    "self_accuracy = nn.evaluate(X, y)  \n",
    "test_accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('epoch:{}, Accuracy self:{}, test:{}%'.\n",
    "      format(epoch, np.round(self_accuracy, 2), test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist()\n",
    "nn = MnistBGD(784, 100, 10, eta = 0.1, epochs = 1000)\n",
    "nn.fit(X, y)\n",
    "accuracy = nn.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "_For God so loved the world that he gave his one and only Son, that whoever believes in him shall not perish but have eternal life. John3:16_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
