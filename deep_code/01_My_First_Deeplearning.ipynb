{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltnysvq8D4e3"
   },
   "source": [
    "# 2장 처음 해보는 딥러닝\n",
    "---------------------------------------\n",
    "\n",
    "## 1. 미지의 일을 예측하는 힘\n",
    "\n",
    "\n",
    "중환자를 전문으로 수술하는 어느 병원 의사가 수많은 환자를 수술해 오던 중 다음과 같은 질문을 가졌다고 합시다. \"혹시 수술하기 전에 수술 후의 생존율을 수치로 예측할 수 있을까?\" 자신이 그동안 집도한 수술 환자의 수술 전 상태와 수술 후 생존율을 정리해 놓은 데이터가 있다면 이러한 예측이 가능합니다.  기존 환자의 자료가 머신러닝에 입력되면, 그 패턴과 규칙이 분석이 되어 어떤 특정한 \"함수\" 혹은 \"프로그램\"이 만들어집니다. 그리고 새로운 환자의 자료를 머신러닝에 입력을 하면 예전의 데이터와 비교하여 생존 가능성이 몇 퍼센트인지 알려 줍니다. \n",
    "이것이 바로 머신러닝이 하는 일입니다.\n",
    "\n",
    "여기서 자료가 입력되고 패턴이 분석되는 과정을 __학습 Training__ 이라고 부릅니다.  이러한 자료들을 분류하고 예측하는 여러 가지 방법이 1950년대부터 학자들이 제안하였습니다. 그 결과 랜덤 포레스트(random forest), 서포트 벡터 머신(support vector machine)등과 방법들이 있습니다. 딥러닝은 그러한 방법들 중에 가장 효과적인 방법입니다.\n",
    "\n",
    "## 2. 폐암 수술 환자의 생존율 예측하기\n",
    "\n",
    "이러한 딥러닝을 코드로 구현하려면, 수 년 전만 하더라도 많은 코딩이 필요했지만, 지금은 단 몇 줄의 코드로도 가능하게 되었습니다. 지금부터 실제로 폴란드 브로츠와프 의과대학에서 2013년에 공개한 폐암 수술 환자의 수술 전 진단 데이터와 수술 후 생존 결과를 기록한 실제 의료 기록 데이터를 가지고 수술환자의 생존율을 예측하는 알고리즘을 구현한 머신 러닝 프로그램은 다음과 같습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deep_code/01_My_First_Deeplearning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 681,
     "status": "error",
     "timestamp": 1550888120753,
     "user": {
      "displayName": "Youngsup Kim",
      "photoUrl": "https://lh5.googleusercontent.com/-VaN9GIdW_Sg/AAAAAAAAAAI/AAAAAAAADPM/BUfUFgx0mIM/s64/photo.jpg",
      "userId": "15460447754328385632"
     },
     "user_tz": -540
    },
    "id": "C5fFXCTwD1wU",
    "outputId": "0827efc8-147e-44b8-964b-97c12da78b88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "470/470 [==============================] - 0s 329us/step - loss: 0.6631 - acc: 0.3106\n",
      "Epoch 2/30\n",
      "470/470 [==============================] - 0s 57us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 3/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 4/30\n",
      "470/470 [==============================] - 0s 66us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 5/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1488 - acc: 0.8511\n",
      "Epoch 6/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 7/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 8/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 9/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 10/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1486 - acc: 0.8511\n",
      "Epoch 11/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1498 - acc: 0.8447\n",
      "Epoch 12/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1487 - acc: 0.8511\n",
      "Epoch 13/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1485 - acc: 0.8511\n",
      "Epoch 14/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1483 - acc: 0.8511\n",
      "Epoch 15/30\n",
      "470/470 [==============================] - 0s 59us/step - loss: 0.1485 - acc: 0.8511\n",
      "Epoch 16/30\n",
      "470/470 [==============================] - 0s 57us/step - loss: 0.1490 - acc: 0.8447\n",
      "Epoch 17/30\n",
      "470/470 [==============================] - 0s 57us/step - loss: 0.1479 - acc: 0.8489\n",
      "Epoch 18/30\n",
      "470/470 [==============================] - 0s 57us/step - loss: 0.1482 - acc: 0.8468\n",
      "Epoch 19/30\n",
      "470/470 [==============================] - 0s 59us/step - loss: 0.1477 - acc: 0.8511\n",
      "Epoch 20/30\n",
      "470/470 [==============================] - 0s 59us/step - loss: 0.1480 - acc: 0.8511\n",
      "Epoch 21/30\n",
      "470/470 [==============================] - 0s 68us/step - loss: 0.1475 - acc: 0.8511\n",
      "Epoch 22/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1469 - acc: 0.8511\n",
      "Epoch 23/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1467 - acc: 0.8511\n",
      "Epoch 24/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1476 - acc: 0.8489\n",
      "Epoch 25/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1471 - acc: 0.8511\n",
      "Epoch 26/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1466 - acc: 0.8511\n",
      "Epoch 27/30\n",
      "470/470 [==============================] - 0s 59us/step - loss: 0.1472 - acc: 0.8511\n",
      "Epoch 28/30\n",
      "470/470 [==============================] - 0s 64us/step - loss: 0.1471 - acc: 0.8489\n",
      "Epoch 29/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1471 - acc: 0.8489\n",
      "Epoch 30/30\n",
      "470/470 [==============================] - 0s 62us/step - loss: 0.1462 - acc: 0.8532\n",
      "470/470 [==============================] - 0s 57us/step\n",
      "\n",
      " Accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "# My_First_Deeplearning.py\n",
    "# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# 준비된 수술 환자 데이터를 불러들입니다.\n",
    "Data_set = numpy.loadtxt(\"../dataset/ThoraricSurgery.csv\", delimiter=\",\")\n",
    "\n",
    "# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장합니다.\n",
    "X = Data_set[:,0:17]\n",
    "Y = Data_set[:,17]\n",
    "\n",
    "# 딥러닝 구조를 결정합니다(모델을 설정하고 실행하는 부분입니다).\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=17, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 딥러닝을 실행합니다.\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=30, batch_size=10)\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vudr2p4TEOhZ"
   },
   "source": [
    "프로그램을 실행한 결과로 가장 아랫줄에 정확도가 출력되어 있습니다.  정확도가 1.0이면 예측 정확도가 100%라는 뜻입니다.  그러므로, 기존 폐암 환자의 데이터를 딥러닝으로 학습시킨 결과를 활용하여, 새로운 환자의 수술 후 생존율을 예측하니 85%정도의 생존률이라는 결과가 나온 것입니다. 80~90%는 생존한다는 것이죠. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 코드 분석\n",
    "\n",
    "위의 코드는 세 부분, 즉 __(1) 데이터 분석과 입력__, __(2) 딥러닝의 실행__, __(3) 결과 출력__ 으로 나눌 수 있습니다. \n",
    "\n",
    "### (1) 데이터 분석과 입력\n",
    "코드는 넘파이(numpy)라는 라이브러리를 불러오는 것으로 시작합니다. 넘파이는 수치 계산을 위해 미리 만들어 놓은 많은 함수가 내장되어 있는 라이브러리입니다. 넘파이에 있는 `loadtxt`라는 함수를 사용하여 `ThoraricSurgery.csv` 라는 외부 파일에 저장되어 있는 데이터를 읽어와서 `Data_set`이라는 임시 저장소(변수)에 모두 저장했습니다.  머신 러닝에서 중요한 일 중에 하나는 데이터를 잘 준비하고, 또한 데이터를 정확히 이해하고 효율적으로 다루는 것입니다. \n",
    "\n",
    "주피터 노트북 Dash Board에서 해당 파일을 더블 클릭하면 바로 읽어올 수 있습니다. 혹은 다음과 같이 주피터 노트북 매직 코맨드를 사용하는 것도 가능합니다. 이 파일을 열어보면 모두 470개 줄로 이루어져 있고, 각 줄은 18개 항목으로 구분되어 있습니다. \n",
    "\n",
    "한 줄 한 줄이 서로 다른 환자의 상태를 기록한 정보입니다.  총 470 행이므로 470명의 환자에 대한 정보입니다. 쉼표로 이어진 항목이 각 줄마다 18개가 있습니다. 이는 환자마다 18개의 정보를 순서에 맞춰 정리한 것이죠. 앞의 17개 정보는 종양의 유형, 폐활량, 호흡 곤란 여부, 고통 정도, 기침, 흡연, 천식 여부 등 17가지의 환자 상태를 조사해서 기록한 것입니다.  그리고, 마지막 18번째 정보는 수술 후 생존 결과입니다. \n",
    "\n",
    "1 또는 0이라는 값이 많이 보이는데, 1은 true, 즉 해당 사항이 있다는 것을 뜻하고, 0는 false라는 값은 해당 사항이 없다는 것을 뜻합니다.  맨 마지막 열인 18번째 항목에서 1은 수술 후 생존했음을, 0은 수술 후 사망했음을 의합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../dataset/ThoraricSurgery.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293,1,3.8,2.8,0,0,0,0,0,0,12,0,0,0,1,0,62,0\n",
      "1,2,2.88,2.16,1,0,0,0,1,1,14,0,0,0,1,0,60,0\n",
      "8,2,3.19,2.5,1,0,0,0,1,0,11,0,0,1,1,0,66,1\n",
      "14,2,3.98,3.06,2,0,0,0,1,1,14,0,0,0,1,0,80,1\n",
      "17,2,2.21,1.88,0,0,1,0,0,0,12,0,0,0,1,0,56,0\n",
      "18,2,2.96,1.67,0,0,0,0,0,0,12,0,0,0,1,0,61,0\n",
      "35,2,2.76,2.2,1,0,0,0,1,0,11,0,0,0,0,0,76,0\n",
      "42,2,3.24,2.52,1,0,0,0,1,0,12,0,0,0,1,0,63,1\n",
      "65,2,3.15,2.76,1,0,1,0,1,0,12,0,0,0,1,0,59,0\n",
      "111,2,4.48,4.2,0,0,0,0,0,0,12,0,0,0,1,0,55,0\n"
     ]
    }
   ],
   "source": [
    "!head ../dataset/ThoraricSurgery.csv       #!tail ../dataset/ThoraricSurgery.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 머신 러닝의 목적은 1번째부터 17번째까지의 항목을 분석하여 18번째 항목, 즉 수술 후 생존 또는 사망을 예측하는 것입니다.  1번째 항목부터 17번째 항목까지를 __속성 attribute__ 이라고 하고, 정답에 해당하는 18번째 항목을 __클래스 class__ 라고 합니다.  딥러닝을 구동시키려면 `속성`만을 뽑아서 데이터셋을 만들고, `클래스`를 담는 데어터셋을 따로 만들어 주어야 합니다 \n",
    "\n",
    "속성 데이터셋 X와 클래스 데이터셋 Y는 다음과 같이 만들 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Data_set[:, 0:17]\n",
    "Y = Data_set[:, 17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 딥러닝 실행\n",
    "우리 교재에서는 주로 __케라스 Keras__ 사용해서 딥러닝을 실행합니다.  케라스가 구동되려면, __텐서플로 TensorFlow__ 가 기본적으로 설치되어 있어야 합니다.  딥러닝 프로젝트를 `여행`에 비유해 본다면, 텐서플로는 목적지까지 빠르게 이동시켜주는 `비행기`에 해당하고, 케라스는 비행기의 이륙 및 정확한 지점까지의 도착을 책임지는 `파일럿`에 비유할 수 있습니다. \n",
    "\n",
    "케라스와 텐서플로가 제대로 설치되어 있다면, 다음 라이브러리를 불러올 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential함수는 딥러닝 모델을 구성하는 층(layer)을 한 층 한 층 쉽게 쌓아 올릴 수 있게 해줍니다. Squential함수를 선언하고, model.add() 함수를 사용해서 필요한 층을 차례로 추가하면 됩니다.  \n",
    "\n",
    "여기 코드에서는 두 개의 층을 다음과 같이 만들었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=17, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 개의 층을 쌓을지는 데이터와 그 목적에 따라 그 때 그 때 결정합니다.  model.add()함수 안에는 Dense() 함수가 포함되어 있습니다.  영어 단어 dense는 `조밀하게 모여있는 집합`이란 뜻으로, 여기서는 각 층이 제각각 어떤 특성을 가질지 옵션을 설정하는 역할을 합니다. \n",
    "\n",
    "딥러닝 모델의 구조와 층별 옵션을 정하고 나면, compile()함수를 이용해 이를 실행시킵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=30, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss, optimizer같은 키워드가 나옵니다.  앞서 나온 activation까지 더하여, 이 몇 줄의 코드에 딥러닝의 핵심이 거의 담겨져 있습니다. \n",
    "\n",
    "- activation: 다음 층으로 어떻게 값을 넘길지 결정하는 부분입니다. 여기서는 가장 많이 사용되는 Relu와 sigmoid 함수를 사용하도록 지정한 것입니다. \n",
    "\n",
    "- loss: 한 번 신경망이 실행될 때마다 오차 값을 추적하는 함수입니다. \n",
    "\n",
    "- optimizer: 오차를 어떻게 줄여 나갈지 정하는 함수입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 부분에서는 model.evaluate()함수를 이용해서 앞서 만든 딥러닝 모델이 어느 정도 정확하게 예측하는지를 점검할 수 있습니다. \n",
    "\n",
    "이 코드를 통해 출력되는 정확도는 학습 대상이 되는 기존 환자들의 데이터 중에 일부를 랜덤하게 추출해, 새 환자인 것처럼 가정하고 테스트한 결과입니다.  좀 더 신뢰할 수 있는 정확도를 측정하려면, 학습 단계에서 미리 일부를 떼어내어 테스트셋으로 저장하고, 테스트할 때는 오직 이 데이터셋만들 사용합니다.  이 부분에 대한 것은 나중에 다시 다루게 됩니다. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "My_First_Deeplearning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
